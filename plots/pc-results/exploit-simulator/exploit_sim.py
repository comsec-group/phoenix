#!/usr/bin/env python3
# -*- coding: UTF-8 -*-
# Copyright (c) 2016 Andrei Tatar
# Copyright (c) 2018 Vrije Universiteit Amsterdam
#
# This program is licensed under the GPL2+.

import sys
import json
from collections import defaultdict

from rhsimulator.sim            import *
from rhsimulator.flip           import *
from exploits.ffs_exploit       import *
from exploits.opcode_sudo       import *
from exploits.x86pte_exploits   import *
from glob import glob

import pandas as pd
from pandas import DataFrame
import pprint as pp

from util.util import get_directory_parts, get_timestamp_str_from_directory_part, extract_timestamp_from_path, extract_context_from_path

def bit(x):
    return 1<<x

# NOTE add to this array what you want to grep for when you're filtering the files
# at the moment grepping for sweep-summary files
# NOTE In the current version, this is not being used., so I commented it out to avoid confusion
# filter_keywords = ['.json','sweep-summary-1x256MB']

if __name__ == '__main__':
    if len(sys.argv) < 3:
        print("Missing arguments")
        print("usage: {} DIR_NAME cpu_type".format(sys.argv[0]))
        sys.exit(1)
    cpu_type = sys.argv[2]
    # if len(sys.argv) == 3:
    #     sweep_summary_identifier = sys.argv[2]
    # else:
    #     sweep_summary_identifier = '256MB'

    # files_grouped_by_dimm = defaultdict(set)
    # found_file = False
    # for f in glob(f"{sys.argv[1]}/**/sweep-summary-1x*.json", recursive=True):
    #     if cpu_type in f:
    #         found_file = True
    #         print(f)
    #         _, _, dimm_id, _ = extract_context_from_path(f)
    #         files_grouped_by_dimm[dimm_id].add(f)
    # if not found_file:
    #     print(f"[ERROR] No files found!")
    #     sys.exit(1)

    # # Take the newest experiments
    # files = []
    # for file_set in files_grouped_by_dimm.values():
    #     files.append(max(file_set, key=extract_timestamp_from_path))

    # #files = [os.path.join(sys.argv[1],x) for x in os.listdir(sys.argv[1])
    # #        if all([key in x for key in filter_keywords])]

    # pp.pprint(files)
    # print("Are these the files you want to test [y/n]? ", end="")
    # answer = input()
    # while answer not in ['y', 'n', 'Y', 'N']:
    #     print("[ERROR] Input not recognized")
    #     print("Are these the files you want to test[y/n]? ", end="")
    #     answer = input()

    # if answer in ['n', 'N']:
    #     print("[ERROR] Not the right files")
    #     sys.exit(1)

    files = []
    for f in glob(f"{sys.argv[1]}/system_sweep*.json", recursive=True):
        files.append(f)

    results = []
    for ffile in files:
        with open(ffile, 'r') as f:
            data = json.load(f)

        metadata = data['metadata']
        print('metadata', metadata)
        #TODO implemente data aggregation with multiple sweeps

        if len(data['sweeps']) > 1:
            print("[ERROR] this script currently supports a single sweep. "
                "Aggregating data among multiple sweeps still need to be implemented")

        sweep = data['sweeps'][0] #extracting the only sweep in the file

        # Check if there is 0 bit flip
        if sweep['flips']['total'] == 0:
            print(f"[WARNING] No bit flips in {ffile}")
            continue

        hardcoded_mem_config = {
            'channels': 1,
            'dimms': 1,
            'ranks': 2,
            'banks': 16,
        }
        if 'sweep-summary-1x2048.json' in ffile:
            hardcoded_mem_config['ranks'] = 1
        metadata['memory_config'] = hardcoded_mem_config

        ftbl = Fliptable.from_sweep(sweep, metadata, layout_ow=True)
        print(f"[+] dimm: {ftbl.dimm_id:>5}")

        # FlipPFN
        expl_name = 'FlipPFN(16GB)'
        est = FliptableEstimator(ftbl)
        est.run_exploit(FlipPFN(16<<30))
        data = est.get_csv_stats(expl_name)
        results.append(data)
        print(f"\t{expl_name:>20}: {data['tot_expl_flip_str']:>40}")

        # OpcodeFlip
        expl_name = 'OpcodeFlip'
        est.run_exploit(OpcodeFlip())
        data = est.get_csv_stats(expl_name)
        results.append(data)
        print(f"\t{expl_name:>20}: {data['tot_expl_flip_str']:>40}")

        # GPGFlip
        expl_name = 'GPGFlip'
        est.run_exploit(GPGFlip())
        data = est.get_csv_stats(expl_name)
        results.append(data)
        print(f"\t{expl_name:>20}: {data['tot_expl_flip_str']:>40}")

    with open(f"export_{cpu_type}.csv", "w+") as f:
        pd.DataFrame(results).to_csv(f)



